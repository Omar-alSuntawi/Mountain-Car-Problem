{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# Set plotting options\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "np.set_printoptions(precision=3, linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment and set random seed\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(505);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score: -200.0\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "score = 0\n",
    "for t in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    score += reward\n",
    "    if done:\n",
    "        break \n",
    "print('Final score:', score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Box(2,)\n",
      "- low: [-1.2  -0.07]\n",
      "- high: [0.6  0.07]\n"
     ]
    }
   ],
   "source": [
    "# Explore state (observation) space\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"- low:\", env.observation_space.low)\n",
    "print(\"- high:\", env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space samples:\n",
      "[[ 0.235 -0.025]\n",
      " [-0.355 -0.006]\n",
      " [ 0.558  0.05 ]\n",
      " [ 0.171 -0.03 ]\n",
      " [-0.766  0.028]\n",
      " [-0.524 -0.065]\n",
      " [-0.021  0.035]\n",
      " [-0.263 -0.048]\n",
      " [ 0.165  0.059]\n",
      " [ 0.049 -0.03 ]]\n"
     ]
    }
   ],
   "source": [
    "# Generate some samples from the state space \n",
    "print(\"State space samples:\")\n",
    "print(np.array([env.observation_space.sample() for i in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(3)\n",
      "Action space samples:\n",
      "[1 0 1 1 1 2 1 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "# Generate some samples from the action space\n",
    "print(\"Action space samples:\")\n",
    "print(np.array([env.action_space.sample() for i in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_uniform_grid(low, high, bins=(10, 10)):\n",
    "    \"\"\"Define a uniformly-spaced grid that can be used to discretize a space.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    low : array_like\n",
    "        Lower bounds for each dimension of the continuous space.\n",
    "    high : array_like\n",
    "        Upper bounds for each dimension of the continuous space.\n",
    "    bins : tuple\n",
    "        Number of bins along each corresponding dimension.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grid : list of array_like\n",
    "        A list of arrays containing split points for each dimension.\n",
    "    \"\"\"\n",
    "    grid = []\n",
    "    for i in range(len(bins)):\n",
    "        step = (high[i] - low[i]) / bins[i]\n",
    "        grid.append(np.linspace(low[i]+step, high[i] ,endpoint=False, num=bins[i]-1))\n",
    "    \n",
    "    return grid\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splits for 10 bins\n",
      " [array([-0.8, -0.6, -0.4, -0.2,  0. ,  0.2,  0.4,  0.6,  0.8]), array([-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.])]\n",
      "__________\n",
      "splits for 15 bins\n",
      " [array([-0.867, -0.733, -0.6  , -0.467, -0.333, -0.2  , -0.067,  0.067,  0.2  ,  0.333,  0.467,  0.6  ,  0.733,  0.867]), array([-4.333, -3.667, -3.   , -2.333, -1.667, -1.   , -0.333,  0.333,  1.   ,  1.667,  2.333,  3.   ,  3.667,  4.333])]\n",
      "__________\n",
      "splits for 5 bins\n",
      " [array([-0.6, -0.2,  0.2,  0.6]), array([-3., -1.,  1.,  3.])]\n"
     ]
    }
   ],
   "source": [
    "# Testing the grid for different bins, default is 10\n",
    "low = [-1.0, -5.0]   \n",
    "high = [1.0, 5.0]\n",
    "grid = create_uniform_grid(low, high)   #Printing the two states\n",
    "print(\"splits for 10 bins\\n\" , grid)\n",
    "print(\"__________\")\n",
    "\n",
    "bins = (15,15)\n",
    "grid = create_uniform_grid(low, high, bins)\n",
    "print(\"splits for 15 bins\\n\" , grid)\n",
    "print(\"__________\")\n",
    "\n",
    "bins = (5,5)\n",
    "grid = create_uniform_grid(low, high, bins)\n",
    "print(\"splits for 5 bins\\n\" , grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(sample, grid):\n",
    "    \"\"\"Discretize a sample as per given grid.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample : array_like\n",
    "        A single sample from the (original) continuous space.\n",
    "    grid : list of array_like\n",
    "        A list of arrays containing split points for each dimension.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    discretized_sample : array_like\n",
    "        A sequence of integers with the same number of dimensions as sample.\n",
    "    \"\"\"\n",
    "    dim = len(np.transpose(sample))\n",
    "    discrete = np.zeros((dim,), dtype=int)\n",
    "    for j in range(dim):\n",
    "            discrete[j] = int(np.digitize(sample[j], grid[j]))\n",
    "            \n",
    "    return discrete\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Samples:\n",
      "array([[-1.  , -5.  ],\n",
      "       [-0.81, -4.1 ],\n",
      "       [-0.8 , -4.  ],\n",
      "       [-0.5 ,  0.  ],\n",
      "       [ 0.2 , -1.9 ],\n",
      "       [ 0.8 ,  4.  ],\n",
      "       [ 0.81,  4.1 ],\n",
      "       [ 1.  ,  5.  ]])\n",
      "\n",
      "Discretized samples for 10 bins:\n",
      "array([[0, 0],\n",
      "       [0, 0],\n",
      "       [1, 1],\n",
      "       [2, 5],\n",
      "       [6, 3],\n",
      "       [9, 9],\n",
      "       [9, 9],\n",
      "       [9, 9]])\n",
      "\n",
      "Samples:\n",
      "array([[-1.  , -5.  ],\n",
      "       [-0.81, -4.1 ],\n",
      "       [-0.8 , -4.  ],\n",
      "       [-0.5 ,  0.  ],\n",
      "       [ 0.2 , -1.9 ],\n",
      "       [ 0.8 ,  4.  ],\n",
      "       [ 0.81,  4.1 ],\n",
      "       [ 1.  ,  5.  ]])\n",
      "\n",
      "Discretized samples for 20 bins:\n",
      "array([[0, 0],\n",
      "       [0, 0],\n",
      "       [1, 1],\n",
      "       [2, 5],\n",
      "       [6, 3],\n",
      "       [9, 9],\n",
      "       [9, 9],\n",
      "       [9, 9]])\n",
      "\n",
      "Samples:\n",
      "array([[-1.  , -5.  ],\n",
      "       [-0.81, -4.1 ],\n",
      "       [-0.8 , -4.  ],\n",
      "       [-0.5 ,  0.  ],\n",
      "       [ 0.2 , -1.9 ],\n",
      "       [ 0.8 ,  4.  ],\n",
      "       [ 0.81,  4.1 ],\n",
      "       [ 1.  ,  5.  ]])\n",
      "\n",
      "Discretized samples for 5 bins:\n",
      "array([[0, 0],\n",
      "       [0, 0],\n",
      "       [1, 1],\n",
      "       [2, 5],\n",
      "       [6, 3],\n",
      "       [9, 9],\n",
      "       [9, 9],\n",
      "       [9, 9]])\n",
      "\n",
      "Samples:\n",
      "array([[-1.  , -5.  ],\n",
      "       [-0.81, -4.1 ],\n",
      "       [-0.8 , -4.  ],\n",
      "       [-0.5 ,  0.  ],\n",
      "       [ 0.2 , -1.9 ],\n",
      "       [ 0.8 ,  4.  ],\n",
      "       [ 0.81,  4.1 ],\n",
      "       [ 1.  ,  5.  ]])\n",
      "\n",
      "Discretized samples for 50 bins:\n",
      "array([[0, 0],\n",
      "       [0, 0],\n",
      "       [1, 1],\n",
      "       [2, 5],\n",
      "       [6, 3],\n",
      "       [9, 9],\n",
      "       [9, 9],\n",
      "       [9, 9]])\n"
     ]
    }
   ],
   "source": [
    "# Test with a simple grid and some samples\n",
    "grid = create_uniform_grid([-1.0, -5.0], [1.0, 5.0])\n",
    "grid_20 = create_uniform_grid([-1.0, -5.0], [1.0, 5.0],(20,20))\n",
    "grid_5 = create_uniform_grid([-1.0, -5.0], [1.0, 5.0], (5,5))\n",
    "grid_50 = create_uniform_grid([-1.0, -5.0], [1.0, 5.0], (50,50))\n",
    "samples = np.array(\n",
    "    [[-1.0 , -5.0],\n",
    "     [-0.81, -4.1],\n",
    "     [-0.8 , -4.0],\n",
    "     [-0.5 ,  0.0],\n",
    "     [ 0.2 , -1.9],\n",
    "     [ 0.8 ,  4.0],\n",
    "     [ 0.81,  4.1],\n",
    "     [ 1.0 ,  5.0]])\n",
    "\n",
    "\n",
    "discretized_samples = np.array([discretize(sample, grid) for sample in samples])\n",
    "print(\"\\nSamples:\", repr(samples), sep=\"\\n\")\n",
    "print(\"\\nDiscretized samples for 10 bins:\", repr(discretized_samples), sep=\"\\n\")\n",
    "\n",
    "discretized_samples_20 = np.array([discretize(sample, grid_20) for sample in samples])\n",
    "print(\"\\nSamples:\", repr(samples), sep=\"\\n\")\n",
    "print(\"\\nDiscretized samples for 20 bins:\", repr(discretized_samples), sep=\"\\n\")\n",
    "\n",
    "discretized_samples_5 = np.array([discretize(sample, grid_5) for sample in samples])\n",
    "print(\"\\nSamples:\", repr(samples), sep=\"\\n\")\n",
    "print(\"\\nDiscretized samples for 5 bins:\", repr(discretized_samples), sep=\"\\n\")\n",
    "\n",
    "discretized_samples_50 = np.array([discretize(sample, grid_50) for sample in samples])\n",
    "print(\"\\nSamples:\", repr(samples), sep=\"\\n\")\n",
    "print(\"\\nDiscretized samples for 50 bins:\", repr(discretized_samples), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.collections as mc\n",
    "\n",
    "def visualize_samples(samples, discretized_samples, grid, low=None, high=None):\n",
    "    \"\"\"Visualize original and discretized samples on a given 2-dimensional grid.\"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    # Show grid\n",
    "    ax.xaxis.set_major_locator(plt.FixedLocator(grid[0]))\n",
    "    ax.yaxis.set_major_locator(plt.FixedLocator(grid[1]))\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # If bounds (low, high) are specified, use them to set axis limits\n",
    "    if low is not None and high is not None:\n",
    "        ax.set_xlim(low[0], high[0])\n",
    "        ax.set_ylim(low[1], high[1])\n",
    "    else:\n",
    "        # Otherwise use first, last grid locations as low, high (for further mapping discretized samples)\n",
    "        low = [splits[0] for splits in grid]\n",
    "        high = [splits[-1] for splits in grid]\n",
    "\n",
    "    # Map each discretized sample (which is really an index) to the center of corresponding grid cell\n",
    "    grid_extended = np.hstack((np.array([low]).T, grid, np.array([high]).T))  # add low and high ends\n",
    "    grid_centers = (grid_extended[:, 1:] + grid_extended[:, :-1]) / 2  # compute center of each grid cell\n",
    "    locs = np.stack(grid_centers[i, discretized_samples[:, i]] for i in range(len(grid))).T  # map discretized samples\n",
    "\n",
    "    ax.plot(samples[:, 0], samples[:, 1], 'o')  # plot original samples\n",
    "    ax.plot(locs[:, 0], locs[:, 1], 's')  # plot discretized samples in mapped locations\n",
    "    ax.add_collection(mc.LineCollection(list(zip(samples, locs)), colors='orange'))  # add a line connecting each original-discretized sample\n",
    "    ax.legend(['original', 'discretized'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default, 10 bins\n",
    "visualize_samples(samples, discretized_samples, grid, low, high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_samples(samples, discretized_samples_5, grid_5, low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_samples(samples, discretized_samples_20, grid_20, low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_samples(samples, discretized_samples_50, grid_50, low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid to discretize the state space\n",
    "state_grid = create_uniform_grid(env.observation_space.low, env.observation_space.high, bins=(10, 10))\n",
    "state_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain some samples from the space, discretize them, and then visualize them\n",
    "state_samples = np.array([env.observation_space.sample() for i in range(10)])\n",
    "discretized_state_samples = np.array([discretize(sample, state_grid) for sample in state_samples])\n",
    "visualize_samples(state_samples, discretized_state_samples, state_grid,\n",
    "                  env.observation_space.low, env.observation_space.high)\n",
    "plt.xlabel('position'); plt.ylabel('velocity');  # axis labels for MountainCar-v0 state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid to discretize the state space\n",
    "state_grid_20 = create_uniform_grid(env.observation_space.low, env.observation_space.high, bins=(20, 20))\n",
    "state_grid\n",
    "\n",
    "# Obtain some samples from the space, discretize them, and then visualize them\n",
    "state_samples = np.array([env.observation_space.sample() for i in range(10)])\n",
    "discretized_state_samples_20 = np.array([discretize(sample, state_grid_20) for sample in state_samples])\n",
    "visualize_samples(state_samples, discretized_state_samples_20, state_grid_20,\n",
    "                  env.observation_space.low, env.observation_space.high)\n",
    "plt.xlabel('position'); plt.ylabel('velocity');  # axis labels for MountainCar-v0 state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid to discretize the state space\n",
    "state_grid_50 = create_uniform_grid(env.observation_space.low, env.observation_space.high, bins=(50, 50))\n",
    "state_grid\n",
    "\n",
    "# Obtain some samples from the space, discretize them, and then visualize them\n",
    "state_samples = np.array([env.observation_space.sample() for i in range(10)])\n",
    "discretized_state_samples_50 = np.array([discretize(sample, state_grid_50) for sample in state_samples])\n",
    "visualize_samples(state_samples, discretized_state_samples_50, state_grid_50,\n",
    "                  env.observation_space.low, env.observation_space.high)\n",
    "plt.xlabel('position'); plt.ylabel('velocity');  # axis labels for MountainCar-v0 state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_discrete(env, num_episodes, alpha=0.85, discount_factor=0.99, bins = (10,10)):\n",
    "    \"\"\"\n",
    "    Q learning algorithm, off-polics TD control. Finds optimal gready policies\n",
    "    Args:\n",
    "    - env: Given environment to solve\n",
    "    - num_episodes: Number of episodes to learn\n",
    "    - alpha: learning rate\n",
    "    - discount factor: weight/importance given to future rewards\n",
    "    - epsilon: probability of taking random action. \n",
    "             We are using decaying epsilon, \n",
    "             i.e high randomness at beginning and low towards end\n",
    "    Returns:\n",
    "    - Optimal Q\n",
    "    \"\"\"\n",
    "     \n",
    "    # decaying epsilon, i.e we will divide num of episodes passed\n",
    "    epsilon = 1.0\n",
    "    score = 0.0\n",
    "    epsilon_decay_rate=0.9995\n",
    "    # create a numpy array filled with zeros \n",
    "    # rows = number of observations & cols = possible actions\n",
    "    \n",
    "    state_grid = create_uniform_grid(env.observation_space.low, env.observation_space.high, bins)\n",
    "    q_table = np.zeros([env.action_space.n, bins[0], bins[1]]) \n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "            # reset the env\n",
    "            scores = []\n",
    "            \n",
    "            total_reward = 0\n",
    "            \n",
    "            \n",
    "            state = env.reset()\n",
    "            discrete_state = np.array(discretize(state, state_grid)) \n",
    "            steps = 0\n",
    "            # itertools.count() has similar to 'while True:'\n",
    "            for t in itertools.count():\n",
    "                # generate a random num between 0 and 1 e.g. 0.35, 0.73 etc..\n",
    "                # if the generated num is smaller than epsilon, we follow exploration policy \n",
    "                if random.uniform(0, 1) < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                    # select a random action from set of all actions\n",
    "                else:\n",
    "                    action = np.argmax(q_table[:, discrete_state[0], discrete_state[1]])\n",
    "                    # if the generated num is greater than epsilon, we follow exploitation policy\n",
    "                    # select an action with highest value for current state\n",
    "                    \n",
    "                next_state, reward, is_finished, info = env.step(action)\n",
    "                discrete_next_state = np.array(discretize(next_state, state_grid))\n",
    "                \n",
    "                # apply selected action, collect values for next_state and reward\n",
    "                old_value = q_table[action, discrete_state[0], discrete_state[1]]\n",
    "                next_max  = np.max(q_table[:, discrete_next_state[0], discrete_next_state[1]])\n",
    "                new_value = old_value+alpha*(reward+(discount_factor*next_max)-old_value)\n",
    "                # Update the Q table, alpha is the learning rate\n",
    "                q_table[action, discrete_state[0], discrete_state[1]] = new_value\n",
    "                \n",
    "                # break if done, i.e. if end of this episode\n",
    "                if is_finished:\n",
    "                    break\n",
    "                # make the next_state into current state as we go for next iteration\n",
    "                state = next_state\n",
    "                discrete_state = np.array(discretize(state, state_grid)) \n",
    "                #env.render()\n",
    "                total_reward += reward\n",
    "                #print(total_reward)\n",
    "                scores.append(total_reward)\n",
    "                steps= steps+1\n",
    "                #print(\"steps\", steps)\n",
    "                \n",
    "                #print(score)\n",
    "            \n",
    "            avg_score = np.mean(scores)\n",
    "            #print(\"avg_score\", avg_score)\n",
    "            # gradualy decay the epsilon\n",
    "            if epsilon > 0.1:\n",
    "                epsilon -= 1.0/num_episodes\n",
    "                \n",
    "    final_s = 0\n",
    "    final_s = avg_score\n",
    "    print(\"Training finished.\\n\")\n",
    "    print(\"Final score is \", final_s)\n",
    "    print(q_table)\n",
    "    \n",
    "    return q_table   # return optimal Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "\n",
      "Final score is  -100.0\n",
      "[[[  0.    -87.577 -91.084 -91.222 -84.964 -89.47  -86.805 -89.263   0.      0.   ]\n",
      "  [-82.757 -90.719 -91.016 -85.139 -85.442 -84.39  -84.433 -92.405 -79.489   0.   ]\n",
      "  [-88.286 -91.298 -80.995 -85.279 -85.332 -85.502 -85.175 -88.422 -79.714   0.   ]\n",
      "  [-84.156 -90.673 -85.649 -85.87  -89.067 -91.403 -85.835 -84.104 -81.608 -83.439]\n",
      "  [-83.208 -90.234 -89.409 -85.548 -92.655 -89.093 -86.311 -84.788 -86.479 -84.115]\n",
      "  [ -2.167 -89.726 -84.539 -84.744 -84.611 -84.574 -85.6   -82.569 -88.251 -85.843]\n",
      "  [  0.    -86.956 -88.986 -90.83  -85.655 -84.954 -90.443 -89.808 -86.607   0.   ]\n",
      "  [  0.      0.    -89.162 -90.443 -90.464 -90.255 -86.587 -62.065 -82.498   0.   ]\n",
      "  [  0.      0.      0.    -86.511 -85.847 -81.439 -56.134 -67.941   0.      0.   ]\n",
      "  [  0.      0.      0.      0.    -84.657 -53.269 -48.459 -16.989   0.      0.   ]]\n",
      "\n",
      " [[  0.    -89.164 -90.994 -91.402 -91.572 -89.786 -91.864 -88.826   0.      0.   ]\n",
      "  [-84.763 -90.652 -80.912 -84.464 -85.196 -85.084 -85.495 -92.546 -88.35    0.   ]\n",
      "  [-87.973 -90.296 -84.869 -85.375 -85.444 -85.518 -85.015 -84.819 -80.249   0.   ]\n",
      "  [-85.242 -90.399 -85.901 -85.269 -89.066 -89.235 -85.404 -84.702 -80.454 -75.59 ]\n",
      "  [-83.484 -90.361 -85.664 -85.48  -88.504 -89.117 -95.25  -83.476 -88.567 -80.849]\n",
      "  [-62.369 -89.929 -90.385 -85.263 -84.519 -84.851 -84.398 -91.052 -92.22  -81.59 ]\n",
      "  [  0.    -87.029 -90.988 -83.709 -85.57  -84.894 -91.046 -89.632 -71.892   0.   ]\n",
      "  [  0.      0.    -84.352 -89.864 -89.872 -89.869 -85.824 -85.359 -65.      0.   ]\n",
      "  [  0.      0.      0.    -88.848 -85.992 -59.879 -55.329 -70.777   0.      0.   ]\n",
      "  [  0.      0.      0.      0.    -65.023 -67.812 -49.199 -20.619   0.      0.   ]]\n",
      "\n",
      " [[  0.    -87.819 -91.183 -83.452 -85.551 -84.799 -87.711 -78.193   0.      0.   ]\n",
      "  [-78.161 -90.697 -90.254 -83.872 -83.377 -85.133 -84.979 -83.17  -86.934   0.   ]\n",
      "  [-85.883 -90.255 -89.754 -85.061 -85.266 -95.277 -85.186 -82.353 -89.478   0.   ]\n",
      "  [-84.402 -83.668 -83.257 -85.96  -88.659 -88.759 -85.614 -84.459 -80.619 -74.768]\n",
      "  [-84.969 -90.234 -82.888 -85.472 -90.643 -90.195 -85.309 -84.786 -87.49  -75.288]\n",
      "  [  0.    -88.667 -85.1   -84.947 -84.582 -84.894 -84.633 -90.001 -89.478 -84.979]\n",
      "  [  0.    -86.02  -90.531 -89.656 -85.617 -84.477 -84.302 -87.129 -86.863   0.   ]\n",
      "  [  0.      0.    -88.005 -89.647 -90.059 -90.336 -84.307 -85.478 -85.912   0.   ]\n",
      "  [  0.      0.      0.    -88.888 -86.346 -51.851 -51.857 -15.516   0.      0.   ]\n",
      "  [  0.      0.      0.      0.    -81.8   -53.07  -48.185 -14.243   0.      0.   ]]]\n"
     ]
    }
   ],
   "source": [
    "Q = q_learning_discrete(env, 700000)\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_algorithm(env, Q, bins = (10,10)):\n",
    "    \"\"\"\n",
    "    Test script for Q function\n",
    "    Args:\n",
    "    - env: Given environment to test Q function\n",
    "    - Q: Q function to verified\n",
    "    Returns:\n",
    "    - Total rewards for one episode\n",
    "    \"\"\"\n",
    "    state_grid = create_uniform_grid(env.observation_space.low, env.observation_space.high, bins)\n",
    "    \n",
    "    state = env.reset()\n",
    "    discrete_state = np.array(discretize(state, state_grid)) \n",
    "    total_reward = 0\n",
    "    scores = []\n",
    "    avg_score = 0\n",
    "    while True:\n",
    "        # selection the action with highest values i.e. best action\n",
    "        action = np.argmax(Q[:, discrete_state[0], discrete_state[1]])\n",
    "        # apply selected action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state_discrete = np.array(discretize(next_state, state_grid))\n",
    "        \n",
    "        # render environment\n",
    "        env.render()\n",
    "        # calculate total reward\n",
    "        total_reward += reward\n",
    "        scores.append(total_reward)\n",
    "        if done:\n",
    "            print(total_reward)\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "        discrete_state = np.array(discretize(state, state_grid))\n",
    "\n",
    "        \n",
    "    avg_score = np.mean(scores)\n",
    "    print(\"avg score\" , avg_score)\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-200.0\n",
      "avg score -100.5\n"
     ]
    }
   ],
   "source": [
    "#test with default parameters\n",
    "test_algorithm(env, Q)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
